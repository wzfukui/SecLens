"""Exploit-DB RSS collector plugin."""
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
from email.utils import parsedate_to_datetime
from pathlib import Path
from typing import List, Sequence
import json
import logging
import re
import xml.etree.ElementTree as ET

import requests

from app.schemas import BulletinCreate, ContentInfo, SourceInfo

LOGGER = logging.getLogger(__name__)
USER_AGENT = "SecLensExploitDBCollector/1.0"
DEFAULT_LIMIT = 25
DEFAULT_FEED_URL = "https://www.exploit-db.com/rss.xml"
STATE_FILE_NAME = ".cursor"


@dataclass
class FeedEntry:
    exploit_id: str
    title: str
    link: str
    description: str | None
    category: str | None
    published_at: datetime
    guid: str | None
    raw: dict[str, str | None]


def _clean_text(value: str | None) -> str | None:
    if value is None:
        return None
    return " ".join(value.split()).strip() or None


def _parse_pub_date(value: str | None) -> datetime | None:
    if not value:
        return None
    try:
        parsed = parsedate_to_datetime(value)
    except (TypeError, ValueError):
        LOGGER.warning("Unable to parse pubDate '%s'", value)
        return None
    if parsed.tzinfo is None:
        parsed = parsed.replace(tzinfo=timezone.utc)
    return parsed.astimezone(timezone.utc)


_TITLE_CATEGORY_RE = re.compile(r"^\s*\[(?P<category>[^\]]+)\]\s*(?P<title>.*)")


def _split_category(title: str) -> tuple[str, str | None]:
    match = _TITLE_CATEGORY_RE.match(title)
    if not match:
        return title, None
    clean_title = match.group("title").strip() or title
    category = match.group("category").strip() or None
    return clean_title, category


def _extract_exploit_id(link: str) -> str:
    segment = link.rstrip("/").split("/")[-1]
    return segment.strip()


class ExploitDBCollector:
    """Handle fetching and normalizing entries from the Exploit-DB RSS feed."""

    def __init__(
        self,
        *,
        session: requests.Session | None = None,
        feed_url: str | None = None,
        state_path: Path | None = None,
    ) -> None:
        self.session = session or requests.Session()
        self.feed_url = feed_url or DEFAULT_FEED_URL
        self.state_path = state_path or Path(__file__).resolve().with_name(STATE_FILE_NAME)
        self.session.headers.update(
            {
                "User-Agent": USER_AGENT,
                "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.8",
            }
        )

    # --- Cursor helpers -------------------------------------------------
    def load_cursor(self) -> datetime | None:
        try:
            raw = self.state_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError:
            return None
        if not raw:
            return None
        try:
            value = datetime.fromisoformat(raw)
        except ValueError:
            LOGGER.warning("Invalid cursor value '%s'; ignoring", raw)
            return None
        if value.tzinfo is None:
            value = value.replace(tzinfo=timezone.utc)
        return value.astimezone(timezone.utc)

    def save_cursor(self, value: datetime) -> None:
        value = value.astimezone(timezone.utc)
        self.state_path.write_text(value.isoformat(), encoding="utf-8")

    # --- Fetch ----------------------------------------------------------
    def fetch_feed(self) -> Sequence[FeedEntry]:
        response = self.session.get(self.feed_url, timeout=30)
        response.raise_for_status()
        text = response.text.lstrip()
        try:
            root = ET.fromstring(text)
        except ET.ParseError as exc:
            raise ValueError("Failed to parse Exploit-DB RSS feed") from exc

        entries: list[FeedEntry] = []
        for item in root.findall("./channel/item"):
            link = _clean_text(item.findtext("link"))
            if not link:
                continue
            title_raw = _clean_text(item.findtext("title")) or link
            title, category = _split_category(title_raw)
            description = _clean_text(item.findtext("description"))
            pub_date = _parse_pub_date(item.findtext("pubDate")) or datetime.now(timezone.utc)
            guid = _clean_text(item.findtext("guid"))
            exploit_id = _extract_exploit_id(link)
            entries.append(
                FeedEntry(
                    exploit_id=exploit_id,
                    title=title,
                    link=link,
                    description=description,
                    category=category,
                    published_at=pub_date,
                    guid=guid,
                    raw={
                        "title": title_raw,
                        "description": description,
                        "link": link,
                        "pubDate": item.findtext("pubDate"),
                        "guid": guid,
                    },
                )
            )
        return entries

    # --- Normalize ------------------------------------------------------
    def normalize(self, entry: FeedEntry) -> BulletinCreate:
        source = SourceInfo(
            source_slug="exploit_db",
            external_id=entry.exploit_id,
            origin_url=entry.link,
        )
        content = ContentInfo(
            title=entry.title,
            summary=entry.description,
            body_text=entry.description,
            published_at=entry.published_at,
            language="en",
        )

        labels: list[str] = []
        if entry.category:
            labels.append(f"exploit_type:{entry.category.lower()}")

        topics = ["exploit"]

        extra: dict[str, object] = {}
        if entry.category:
            extra["category"] = entry.category
        if entry.guid:
            extra["guid"] = entry.guid

        return BulletinCreate(
            source=source,
            content=content,
            severity=None,
            fetched_at=datetime.now(timezone.utc),
            labels=labels,
            topics=topics,
            extra=extra or None,
            raw={"feed_entry": entry.raw},
        )

    # --- Collection -----------------------------------------------------
    def collect(self, *, limit: int | None = None, force: bool = False) -> List[BulletinCreate]:
        limit = limit or DEFAULT_LIMIT
        cursor = None if force else self.load_cursor()
        entries = list(self.fetch_feed())
        entries.sort(key=lambda item: item.published_at)

        selected: list[FeedEntry] = []
        for entry in entries:
            if cursor and entry.published_at <= cursor:
                continue
            selected.append(entry)
        if limit is not None and limit > 0:
            selected = selected[-limit:]

        bulletins: list[BulletinCreate] = []
        latest = cursor
        for entry in selected:
            bulletin = self.normalize(entry)
            bulletins.append(bulletin)
            if latest is None or entry.published_at > latest:
                latest = entry.published_at

        if latest and not force and bulletins:
            self.save_cursor(latest)
        return bulletins


def run(
    ingest_url: str | None = None,
    token: str | None = None,
    *,
    limit: int | None = None,
    force: bool = False,
) -> tuple[list[BulletinCreate], dict | None]:
    """Entrypoint for the Exploit-DB collector plugin."""

    collector = ExploitDBCollector()
    bulletins = collector.collect(limit=limit, force=force)
    response_data = None
    if ingest_url and bulletins:
        session = requests.Session()
        headers = {"Content-Type": "application/json"}
        if token:
            headers["Authorization"] = f"Bearer {token}"
        session.headers.update(headers)
        payload = [b.model_dump(mode="json") for b in bulletins]
        response = session.post(ingest_url, json=payload, timeout=30)
        response.raise_for_status()
        try:
            response_data = response.json()
        except json.JSONDecodeError:
            response_data = {"status_code": response.status_code}
    return bulletins, response_data


__all__ = ["ExploitDBCollector", "FeedEntry", "run", "DEFAULT_FEED_URL"]
